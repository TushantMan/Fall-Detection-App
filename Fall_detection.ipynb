{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHYrUhRr92Dx5BQOruTrEN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TushantMan/Fall-Detection-App/blob/docker/Fall_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQhOyKn2ZL4Y",
        "outputId": "1e874dd7-7dc5-4799-9a37-7dcbbb90e415"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the working directory\n",
        "import os\n",
        "SKELETON_DIR = '/content/drive/MyDrive/rudder_preTrainModel_with_testset'\n",
        "os.chdir(SKELETON_DIR)\n",
        "! mkdir -p \"$SKELETON_DIR/saved_models\"\n",
        "! mkdir -p \"$SKELETON_DIR/logs\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up auto-reloading modules from the working directory\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "# Install extra dependencies\n",
        "!pip install -q wandb==0.15.0\n",
        "!pip install -q torchmetrics==0.11.3\n",
        "\n",
        "# Set the default figure size\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 120"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjNQE1WyZs5u",
        "outputId": "76262ec0-cb2a-4797-96f3-6efbf9802553"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir(SKELETON_DIR)\n",
        "!pwd\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv_x7gcTZyEo",
        "outputId": "f0c94b49-49b7-4fcc-8b4b-f36e6bd8770e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/rudder_preTrainModel_with_testset\n",
            " dataset.py\t\t    Instructions.docx\t\t 'Predict Labels.csv'   test.py\n",
            " Dataset_test\t\t    logs\t\t\t  __pycache__\t        Untitled14.ipynb\n",
            "'Ground Truth Labels.csv'   model_with_architecture.pth   saved_models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dataset\n",
        "import test\n",
        "\n"
      ],
      "metadata": {
        "id": "-c8X0SoMa0Sx"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L3D6pErQbawb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from itertools import groupby\n",
        "from sklearn import metrics\n",
        "import pandas as pd\n",
        "from dataset import radar_dataset\n",
        "\n",
        "class FlexibleDummyModel(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(FlexibleDummyModel, self).__init__()\n",
        "        self.conv = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(32 * 32 * 31, 32 * num_classes)\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(f\"Input shape: {x.shape}\")\n",
        "        if len(x.shape) == 5:\n",
        "            batch_size, num_images, c, h, w = x.size()\n",
        "            x = x.view(batch_size * num_images, c, h, w)\n",
        "        elif len(x.shape) == 4:\n",
        "            batch_size, c, h, w = x.size()\n",
        "            num_images = 1\n",
        "        # Handle the case with 6 dimensions\n",
        "        elif len(x.shape) == 6:\n",
        "            batch_size, _, num_images, c, h, w = x.size()\n",
        "            x = x.view(batch_size * num_images, c, h, w)\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected input shape: {x.shape}\")\n",
        "\n",
        "        x = self.conv(x)\n",
        "        print(f\"After conv shape: {x.shape}\")\n",
        "        x = x.view(batch_size, num_images, -1)\n",
        "        x = x.mean(dim=1)\n",
        "        print(f\"Before fc shape: {x.shape}\")\n",
        "        x = self.fc(x)\n",
        "        print(f\"After fc shape: {x.shape}\")\n",
        "        return x.view(batch_size, 32, self.num_classes)\n",
        "\n",
        "def run_test_loop(model, testloader, criterion, device):\n",
        "    model.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    test_labels = []\n",
        "    test_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, data in enumerate(testloader, 0):\n",
        "            images_test, labels_test = data\n",
        "            print(f\"Batch {i}: images shape: {images_test[0].shape}, labels shape: {labels_test.shape}\")\n",
        "\n",
        "            # Handle the case where images_test is a list and convert to a tensor\n",
        "            if isinstance(images_test, list):\n",
        "                images_test = torch.stack(images_test, dim=1).float() # stack along the second dimension (num_images)\n",
        "            else:\n",
        "                images_test = images_test.float()\n",
        "\n",
        "            images_test, labels_test = images_test.to(device), labels_test.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            pred_test = model(images_test)\n",
        "            pred_test = pred_test.permute(1, 0, 2)\n",
        "\n",
        "            _, max_index = torch.max(pred_test, dim=2)\n",
        "\n",
        "            for k in range(images_test.size(0)):\n",
        "                raw_prediction = list(max_index[:, k].cpu().numpy())\n",
        "                prediction = [c for c, _ in groupby(raw_prediction) if c != 0]\n",
        "                prediction = prediction[:15]  # Truncate to match label length\n",
        "                prediction += [0] * (15 - len(prediction))  # Pad if necessary\n",
        "\n",
        "                label = labels_test[k].cpu().numpy()\n",
        "                correct += sum(p == l for p, l in zip(prediction, label))\n",
        "                test_predictions.extend(prediction)\n",
        "                test_labels.extend(label)\n",
        "                total += 15\n",
        "\n",
        "            # Debug print\n",
        "            if i == 0:\n",
        "                print(f\"Sample prediction: {prediction}\")\n",
        "                print(f\"Sample label: {label}\")\n",
        "\n",
        "            # Break after first batch for debugging\n",
        "            # break  # Remove this line if you want to run on the full test set\n",
        "\n",
        "    test_accuracy = 100 * correct / total\n",
        "    print(f\"\\nAccuracy on test data: {test_accuracy:.2f}%\")\n",
        "\n",
        "    # More debug information\n",
        "    print(f\"Unique values in predictions: {np.unique(test_predictions)}\")\n",
        "    print(f\"Unique values in labels: {np.unique(test_labels)}\")\n",
        "\n",
        "    pd.DataFrame(test_labels).to_csv('Ground Truth Labels.csv')\n",
        "    pd.DataFrame(test_predictions).to_csv('Predict Labels.csv')\n",
        "\n",
        "    test_acc = metrics.accuracy_score(test_labels, test_predictions)\n",
        "    recall = metrics.recall_score(test_labels, test_predictions, average='macro')\n",
        "    precision = metrics.precision_score(test_labels, test_predictions, average='macro')\n",
        "    f1 = metrics.f1_score(test_labels, test_predictions, average='macro')\n",
        "\n",
        "    print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "    print(f\"Test Recall: {recall:.4f}\")\n",
        "    print(f\"Test Precision: {precision:.4f}\")\n",
        "    print(f\"Test F1 Score: {f1:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    im_size_h, im_size_w = 32, 31\n",
        "    vgg_data_transform = transforms.Compose([\n",
        "        transforms.Resize((im_size_h, im_size_w)),\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    test_dir = '/content/drive/MyDrive/rudder_preTrainModel_with_testset/Dataset_test'  # Adjust this path if needed\n",
        "    testset = radar_dataset(test_dir, 'test', ['radar1_0'], transform=vgg_data_transform)\n",
        "    testloader = DataLoader(testset, batch_size=1, shuffle=False, num_workers=0)\n",
        "\n",
        "    # Check the label range in your dataset\n",
        "    all_labels = [label for _, label in testset]\n",
        "    unique_labels = torch.unique(torch.cat(all_labels))\n",
        "    num_classes = len(unique_labels)\n",
        "    print(f\"Number of unique classes in dataset: {num_classes}\")\n",
        "    print(f\"Unique labels: {unique_labels}\")\n",
        "\n",
        "    model = FlexibleDummyModel(num_classes=num_classes).to(device)\n",
        "    criterion = nn.CTCLoss(reduction='mean', zero_infinity=True, blank=0)\n",
        "\n",
        "    run_test_loop(model, testloader, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1wNU6Z1ftSW",
        "outputId": "3bbdfef6-5e19-48b8-b59e-00eb130bba17"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Number of unique classes in dataset: 4\n",
            "Unique labels: tensor([1, 6, 7, 8])\n",
            "Batch 0: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Sample prediction: [3, 3, 1, 2, 1, 2, 3, 2, 1, 2, 1, 1, 1, 2, 0]\n",
            "Sample label: [1 1 1 1 1 7 7 7 7 7 1 1 1 1 1]\n",
            "Batch 1: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 2: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 3: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 4: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 5: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 6: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 7: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 8: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 9: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 10: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 11: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 12: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 13: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 14: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 15: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 16: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 17: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 18: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 19: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 20: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 21: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 22: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 23: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 24: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 25: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 26: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 27: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 28: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 29: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 30: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 31: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 32: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 33: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 34: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 35: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 36: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 37: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 38: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "Batch 39: images shape: torch.Size([1, 1, 3, 32, 31]), labels shape: torch.Size([1, 15])\n",
            "Input shape: torch.Size([1, 1, 1, 3, 32, 31])\n",
            "After conv shape: torch.Size([1, 32, 32, 31])\n",
            "Before fc shape: torch.Size([1, 31744])\n",
            "After fc shape: torch.Size([1, 128])\n",
            "\n",
            "Accuracy on test data: 19.00%\n",
            "Unique values in predictions: [0 1 2 3]\n",
            "Unique values in labels: [1 6 7 8]\n",
            "Test Accuracy: 0.1900\n",
            "Test Recall: 0.0469\n",
            "Test Precision: 0.0900\n",
            "Test F1 Score: 0.0617\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Dataset and Model Setup:\n",
        "   - It imports necessary libraries and the custom `radar_dataset` class.\n",
        "   - Defines a `FlexibleDummyModel` class that can handle different input shapes.\n",
        "   - Sets up the device (CPU or GPU) for computation.\n",
        "\n",
        "2. Data Preparation:\n",
        "   - Creates a data transformation pipeline using `torchvision.transforms`.\n",
        "   - Loads the test dataset using the custom `radar_dataset` class.\n",
        "   - Creates a DataLoader for batch processing.\n",
        "\n",
        "3. Model Initialization:\n",
        "   - Analyzes the dataset to determine the number of unique classes.\n",
        "   - Initializes the `FlexibleDummyModel` with the correct number of classes.\n",
        "\n",
        "4. Test Loop (run_test_loop function):\n",
        "   - Iterates through the test dataset batch by batch.\n",
        "   - For each batch:\n",
        "     - Preprocesses the input data (handling different shapes).\n",
        "     - Passes the data through the model to get predictions.\n",
        "     - Compares predictions with ground truth labels.\n",
        "     - Accumulates correct predictions and total samples.\n",
        "\n",
        "5. Metrics Calculation:\n",
        "   - Calculates overall accuracy.\n",
        "   - Uses scikit-learn to compute additional metrics:\n",
        "     - Accuracy\n",
        "     - Recall\n",
        "     - Precision\n",
        "     - F1 Score\n",
        "\n",
        "6. Output and Logging:\n",
        "   - Prints the shape of input data and intermediate results for debugging.\n",
        "   - Displays a sample prediction and its corresponding label.\n",
        "   - Prints the calculated metrics.\n",
        "   - Saves the ground truth labels and predictions to CSV files.\n",
        "\n",
        "Key Points:\n",
        "- The script is designed to work with a custom radar dataset, likely containing multiple images per sample.\n",
        "- It uses a dummy model (not trained) to process the data, so the actual performance metrics are not expected to be meaningful.\n",
        "- The main purpose is to ensure that data can be correctly loaded, processed through a model, and evaluated, setting up a framework for when you use a real, trained model.\n",
        "\n",
        "This script essentially serves as a diagnostic tool and a starting point for your radar data classification task. It allows you to verify that:\n",
        "1. Your dataset is loaded correctly.\n",
        "2. The data can be properly fed into a neural network model.\n",
        "3. Predictions can be generated and compared against ground truth labels.\n",
        "4. Performance metrics can be calculated.\n",
        "\n",
        "Later, we will replace the dummy model with our actual trained model (model_with_architecture.pth) to get meaningful classification results."
      ],
      "metadata": {
        "id": "veeKhN0Yg9dd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "45DTGH9ThPVc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}